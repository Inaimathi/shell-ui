#!/usr/bin/python3

import logging
import os
import re
import time
from optparse import OptionParser
from subprocess import call

import lxml.html
import requests

from selenium import webdriver
from selenium.common.exceptions import ElementClickInterceptedException
from selenium.webdriver.support.ui import WebDriverWait

logging.basicConfig()
log = logging.getLogger("stream")

INDEX_FNAMES = False
INDEX_OFFSET = 0
FORCE = False

headers = {
    "Connection": "keep-alive",
    "Cache-Control": "max-age=0",
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "gzip,deflate,sdch",
    "Accept-Language": "en-US,en;q=0.8",
    "Accept-Charset": "ISO-8859-1,utf-8;q=0.7,*;q=0.3",
}


def _forceClick(driver, elem):
    try:
        elem.click()
    except ElementClickInterceptedException as e:
        obscuring_tag = re.search("another element (<.*?>) obscures", e.msg).group(1)
        obscuring_elem_id = re.search("id=[\"'](.*?)[\"']", obscuring_tag).group(1)
        driver.execute_script(
            f"document.getElementById('{obscuring_elem_id}').remove()"
        )
        _forceClick(driver, elem)


def seleniumStream(
    uri, selector, nextLink=False, headers=headers, delay=False, driver=None
):
    if driver is None:
        driver = webdriver.Firefox()
    if uri is not None:
        driver.get(uri)

    while True:
        log.debug(f"AT  {driver.current_url}")
        for el in driver.find_elements_by_css_selector(selector):
            elem_link = el.get_attribute("src") or el.get_attribute("href")
            yield __cleanUri(uri, elem_link)
            __wait(delay)

        try:
            nxt = driver.find_element_by_css_selector(nextLink)
        except Exception:
            nxt = None
        if nxt:
            _forceClick(driver, nxt)
            WebDriverWait(driver, 10)
        else:
            driver.close()
            break


def urlStream(uri, selector, nextLink=False, headers=headers, delay=False):
    cookies = {}
    cur_uri = uri
    while cur_uri:
        log.debug(f"AT  {cur_uri}")
        pg = lxml.html.fromstring(
            requests.get(cur_uri, headers=headers, cookies=cookies).content
        )
        for elem in pg.cssselect(selector):
            yield __cleanUri(uri, __elem_uri(elem))
            __wait(delay)
        try:
            link = __cleanUri(uri, __elem_uri(pg.cssselect(nextLink)[0]))
            log.debug("Found next: " + link)
            if cur_uri != link:
                cur_uri = link
            else:
                cur_uri = False
        except Exception:
            log.debug("Some kind of error in calling nextLink...")
            cur_uri = False


# Internal Utility
def __elem_uri(elem):
    return (
        elem.get("data-super-full-img")
        or elem.get("data-download-uri")
        or elem.get("src")
        or elem.get("href")
    )


def __cleanUri(base, uri):
    base = base.split("/")[0:3]
    uri = uri.strip()
    if len(re.findall("https?://", uri)) > 1:
        return re.findall("https?://.*$", uri)[0].strip()
    elif uri.startswith("http"):
        return uri
    elif uri.startswith("//"):
        return base[0] + uri
    elif uri.startswith("/"):
        return "/".join(base) + uri
    else:
        return "/".join(base + [uri])


def __wait(secs=False):
    if secs:
        log.info("Waiting: %d", secs)
        time.sleep(secs)


def __fnameFromUri(uri):
    return uri.split("/")[-1]


# Retrievers
def __requestsRetrieve(uri, fname):
    with open(fname, "wb") as dest:
        dest.write(requests.get(uri, headers=headers).content)


def __wgetRetrieve(uri, fname):
    call(["wget", "-O", fname, uri])


def __newFromUri(getter, uri, fname=False):
    if not fname:
        fname = __fnameFromUri(uri)
    if (not FORCE) and os.path.exists(fname):
        log.warn("Already got: %s", fname)
    elif (not FORCE) and os.path.exists(__fnameFromUri(uri)):
        log.warn("Already got: %s as %s", fname, __fnameFromUri(uri))
    else:
        log.log(25, "Getting %s ...", fname)
        getter(uri, fname)


def processStream(genOfUris, notify=10, dir=None, should_index=False, index=0):
    if dir is not None:
        if not os.path.isabs(dir):
            dir = os.path.join(os.getcwd(), dir)
        try:
            os.chdir(dir)
        except OSError:
            os.mkdir(dir)
            os.chdir(dir)
    if genOfUris is None:
        return None
    for i, uri in enumerate(genOfUris):
        if i % notify == 0 and i > 0:
            log.info(f"Got {i} things...")
        fname = __fnameFromUri(uri)
        if should_index:
            fname = str(i + index).zfill(5) + "-" + fname
        __newFromUri(__requestsRetrieve, uri, fname)


####################
# ### main thing ###
####################

if __name__ == "__main__":
    parser = OptionParser()
    parser.add_option(
        "-D",
        "--directory",
        dest="directory",
        default=None,
        help="Specify a directory to possibly create and download the pictures into",
    )
    parser.add_option(
        "-S",
        "--selenium",
        dest="selenium",
        default=False,
        action="store_true",
        help="Use selenium to produce the image stream (uses more memory and time, but hits JS-enabled pages)",
    )
    parser.add_option(
        "-s",
        "--selector",
        dest="selector",
        default=False,
        help="Manually specify the CSS selector to use for scraping elements from a gallery.",
    )
    parser.add_option(
        "-n",
        "--next",
        dest="next",
        default=False,
        help="Manually specify the CSS selector to use when looking for a next-page link.",
    )
    parser.add_option(
        "-i",
        "--index",
        dest="index",
        default=False,
        action="store_true",
        help="If passed, prepends a numeric index to each scraped filename, padded to 5 zeros.",
    )
    parser.add_option(
        "--si",
        "--starting-index",
        dest="start_at",
        default=0,
        help="Set the starting index value",
    )
    parser.add_option(
        "-d",
        "--delay",
        dest="delay",
        default=False,
        help="Manually specify the number of seconds to pause before downloading each target.",
    )
    parser.add_option(
        "-v",
        "--verbose",
        dest="verbose",
        default=False,
        action="store_true",
        help="If passed, shows extended logging information while downloading.",
    )
    parser.add_option(
        "-q",
        "--quiet",
        dest="quiet",
        default=False,
        action="store_true",
        help="If passed, scrapes silently.",
    )
    parser.add_option(
        "-f",
        "--force",
        dest="force",
        default=False,
        action="store_true",
        help="If passed, doesn't check for duplicate images.",
    )
    (options, args) = parser.parse_args()

    if options.index:
        INDEX_FNAMES = True
    if options.start_at:
        INDEX_OFFSET = int(options.start_at)

    if options.force:
        FORCE = True

    if options.verbose:
        log.setLevel(logging.DEBUG)
    elif options.quiet:
        log.setLevel(logging.CRITICAL)
    else:
        log.setLevel(25)

    for uri in args:
        if not options.selector:
            print("-s/--selector option is required")
        if options.selenium:
            S = seleniumStream
        else:
            S = urlStream
        stream = S(
            uri, options.selector, nextLink=options.next, delay=int(options.delay)
        )
        processStream(
            stream,
            dir=options.directory,
            should_index=options.index,
            index=int(options.start_at) if options.start_at else 0,
        )
