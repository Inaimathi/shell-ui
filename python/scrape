#!/usr/bin/python

import sys, os, re, time
import urllib, requests, cssselect, lxml.html
import logging

from optparse import OptionParser
from subprocess import call

logging.basicConfig()
log = logging.getLogger("picstream")

headers = {
    'Connection': "keep-alive",
    'Cache-Control': "max-age=0",
    'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4",
    'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    'Accept-Encoding': "gzip,deflate,sdch",
    'Accept-Language': "en-US,en;q=0.8",
    'Accept-Charset': "ISO-8859-1,utf-8;q=0.7,*;q=0.3"
}

def getStream(uri, selector, nextLink=False, headers=headers, delay=False):
    cookies = {}
    cur_uri = uri
    while cur_uri:
        pg = __getHtml(cur_uri, cookies=cookies, headers=headers)
        uri_base = uri.split("/")[0:3]
        ## yield cleaned URI
        ##   (we need to handle relative, absolute, protocol-defaulting AND full URIs)
        for elem in pg.cssselect(selector):
            uri = elem.get("data-download-uri") or elem.get("src") or elem.get("href")
            __wait(delay)
            yield __cleanUri(uri_base, uri)
        try:
            link = __cleanUri(uri_base, nextLink(pg))
            cur_uri = link if cur_uri != link else False
        except:
            cur_uri = False

##### Internal Utility
def __cleanUri(base, uri):
    if uri.startswith("http"):
        return uri
    elif uri.startswith("//"):
        return base[0] + uri
    elif uri.startswith("/"):
        return "/".join(base) + uri
    else:
        return "/".join(base + [uri])

def __getHtml(uri, headers=headers, cookies={}):
    return lxml.html.fromstring(requests.get(uri, headers=headers, cookies=cookies).content)

def __wait(secs=False):
    if secs:
        log.info("Waiting: %d", secs)
        time.sleep(secs)

##### Retrievers
def __requestsRetrieve(uri, fname):
    with open(fname, 'wb') as dest:
        dest.write(requests.get(uri, headers=headers).content)

def __curlRetrieve(uri, fname):
    with open(fname, 'wb') as dest:
        dest.write(check_output(["curl", "--socks4", "socks://localhost:9050", uri]))

def __wgetRetrieve(uri, fname):
    call(["wget", "-O", fname, uri])

def __newFromUri(getter, uri, fname=False):
    if not fname:
        fname = uri.split("/")[-1]
    if os.path.exists(fname):
        log.warn("Already got: %s", fname)
    else:
        log.log(25, "Getting %s ...", fname)
        getter(uri, fname)
        log.info("Got it.")

def __getVid(uri):
    call(["get_flash_videos", "-r", options.quality, uri])

def __getPic(uri):
    __newFromUri(__requestsRetrieve, uri)

def processStream(genOfUris, notify=10, getter=__getPic):
    for i, uri in enumerate(genOfUris):
        if i%notify == 0 and i > 0:
            log.info("Got %d things...", i)
        getter(uri)

####################
##### main thing ###
####################
def main(uri):
    if "youtube" in uri:
        if "list=" in uri:
            log.info("Scraping YouTube playlist...")
            processStream(getStream(uri, "ol.playlist-videos-list > li > a"), __getVid)
        else:
            log.info("Scraping YouTube video...")
            __getVid(uri)
    elif "zero-punctuation" in uri:
        log.info("Scraping Zero Punctuation video...")
        __getVid(uri)
    elif "tumblr" in uri:
        log.info("Scraping tumblr...")
        processStream(getStream(uri, "div.photo_holder img"))
    elif "4chan" in uri:
        log.info("Scraping 4chan thread...")
        processStream(getStream(uri, "a.fileThumb"))

def manual(uri, selector, nextLink=False, delay=False):
    log.info("Scraping manual gallery...")
    processStream(getStream(uri, selector, nextLink=nextLink, delay=delay))

if __name__ == "__main__":
    parser = OptionParser()
    parser.add_option("-s", "--selector", dest="selector", default=False,
                      help="Manually specify the CSS selector to use for scraping elements from a gallery.")
    parser.add_option("-n", "--next", dest="next", default=False,
                      help="Manually specify the CSS selector to use when looking for a next-page link.")
    parser.add_option("-d", "--delay", dest="delay", default=False,
                      help="Manually specify the number of seconds to pause before downloading each target.")
    parser.add_option("-v", "--verbose", dest="verbose", default=False, action="store_true",
                      help="If passed, shows extended logging information while downloading.")
    parser.add_option("-q", "--quiet", dest="quiet", default=False, action="store_true",
                      help="If passed, scrapes silently.")
    parser.add_option("-r", "--quality", dest="quality", default="medium",
                      help="""Specify the quality of downloaded videos. Passed on to get_flash_videos. 
Valid options are:

-   high
-   medium
-   low

-   1080p (1920x1080)
-   720p (1280x720)
-   576p (720x576)
-   480w (854x480)
-   480p (640x480)
-   240w (427x240)
-   240p (320x240)

""")
    (options, args) = parser.parse_args()

    if options.verbose:
        log.setLevel(logging.DEBUG)
    elif options.quiet:
        log.setLevel(logging.CRITICAL)
    else:
        log.setLevel(25)

    for uri in args:
        if options.selector:
            manual(uri, options.selector, nextLink=lambda pg: pg.cssselect(options.next)[0].get("href"), delay=options.delay)
        else:
            main(uri)
