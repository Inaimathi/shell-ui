#!/usr/bin/python3

import sys, os, re, time, datetime
import urllib, requests, cssselect, lxml.html, json
import logging

from optparse import OptionParser
from subprocess import call

logging.basicConfig()
log = logging.getLogger("stream")

index_fnames = False
index_offset = 0
timestamp_fnames = False

headers = {
    "Connection": "keep-alive",
    "Cache-Control": "max-age=0",
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "gzip,deflate,sdch",
    "Accept-Language": "en-US,en;q=0.8",
    "Accept-Charset": "ISO-8859-1,utf-8;q=0.7,*;q=0.3",
}


def getStream(uri, selector, nextLink=False, headers=headers, delay=False):
    cookies = {}
    cur_uri = uri
    while cur_uri:
        pg = __getHtml(cur_uri, cookies=cookies, headers=headers)
        uri_base = uri.split("/")[0:3]
        for elem in pg.cssselect(selector):
            __wait(delay)
            yield __cleanUri(uri_base, __elem_uri(elem))
        try:
            link = __cleanUri(uri_base, __elem_uri(pg.cssselect(nextLink)[0]))
            log.debug("Found next: " + link)
            if cur_uri != link:
                cur_uri = link
            else:
                cur_uri = False
        except Exception:
            log.debug("Some kind of error in calling nextLink...")
            cur_uri = False


def getImgurStream(galleryUri, startPage=0):
    uri_base = galleryUri.split("/")[0:3]
    for i in __numbers(startPage):
        pg = __getJson(galleryUri + "/new/page/" + str(i) + "/hit.json")
        if pg and (pg["status"] == 200) and (len(pg["data"]) > 0):
            for img in pg["data"]:
                ext = img["ext"].split("?")[0]
                yield f"{uri_base[0]}//imgur.com/{img['hash']}{ext}"
        else:
            break


# Internal Utility
def __elem_uri(elem):
    return (
        elem.get("data-super-full-img")
        or elem.get("data-download-uri")
        or elem.get("src")
        or elem.get("href")
    )


def __numbers(startFrom, by=lambda n: n + 1):
    n = startFrom
    while True:
        yield n
        n = by(n)


def __cleanUri(base, uri):
    uri = uri.strip()
    if len(re.findall("https?://", uri)) > 1:
        return re.findall("https?://.*$", uri)[0].strip()
    elif uri.startswith("http"):
        return uri
    elif uri.startswith("//"):
        return base[0] + uri
    elif uri.startswith("/"):
        return "/".join(base) + uri
    else:
        return "/".join(base + [uri])


def __getHtml(uri, headers=headers, cookies={}):
    return lxml.html.fromstring(
        requests.get(uri, headers=headers, cookies=cookies).content
    )


def __getJson(uri, headers=headers, cookies={}):
    cont = requests.get(uri, headers=headers, cookies=cookies).content
    return json.loads(cont)


def __wait(secs=False):
    if secs:
        log.info("Waiting: %d", secs)
        time.sleep(secs)


def __fnameFromUri(uri):
    return uri.split("/")[-1]


# Retrievers
def __requestsRetrieve(uri, fname):
    with open(fname, "wb") as dest:
        dest.write(requests.get(uri, headers=headers).content)


def __curlRetrieve(uri, fname):
    with open(fname, "wb") as dest:
        cmd = ["curl", "--socks4", "socks://localhost:9050", uri]
        dest.write(check_output(cmd))


def __wgetRetrieve(uri, fname):
    call(["wget", "-O", fname, uri])


def __newFromUri(getter, uri, fname=False):
    if not fname:
        fname = __fnameFromUri(uri)
    if os.path.exists(fname):
        log.warn("Already got: %s", fname)
    elif os.path.exists(__fnameFromUri(uri)):
        log.warn("Already got: %s as %s", fname, __fnameFromUri(uri))
    else:
        log.log(25, "Getting %s ...", fname)
        getter(uri, fname)
        log.info("Got it.")


def __getVids(uri):
    try:
        f = options.format
    except Exception:
        f = "[width<=2160]"
    call(["youtube-dl", "-c", "-f", f, uri])


def __getPic(uri, fname=False):
    __newFromUri(__requestsRetrieve, uri, fname)


def processStream(genOfUris, notify=10, getter=__getPic, dir=None):
    if dir is not None:
        if not os.path.isabs(dir):
            dir = os.path.join(os.getcwd(), dir)
        try:
            os.chdir(dir)
        except OSError:
            os.mkdir(dir)
            os.chdir(dir)
    if genOfUris is None:
        return None
    for i, uri in enumerate(genOfUris):
        if i % notify == 0 and i > 0:
            log.info("Got %d things...", i)
        fname = __fnameFromUri(uri)
        if index_fnames:
            fname = str(i + index_offset).zfill(5) + "-" + fname
        if timestamp_fnames:
            stamp = re.sub(" ", "_", str(datetime.datetime.now()))
            fname = f"{stamp}___{fname}"
        getter(uri, fname)


####################
# ### main thing ###
####################
specials = {
    "imgur": getImgurStream,
    "youtube": __getVids,
    "zero-punctuation": __getVids,
}

sites = {
    "tumblr": ("div.photo_holder", None, None),
    "gfycat": ("source#webmSource", None, None),
}


def main(uri):
    for k in specials:
        if k in uri:
            return specials[k](uri)
    for k in sites:
        if k in uri:
            s = sites[k]
            return getStream(uri, s[0], nextLink=s[1], delay=s[2])
    print("No appropriate template found in site tables", uri)


if __name__ == "__main__":
    parser = OptionParser()
    parser.add_option(
        "-D",
        "--directory",
        dest="directory",
        default=None,
        help="Specify a directory to possibly create and download the pictures into",
    )
    parser.add_option(
        "-s",
        "--selector",
        dest="selector",
        default=False,
        help="Manually specify the CSS selector to use for scraping elements from a gallery.",
    )
    parser.add_option(
        "-n",
        "--next",
        dest="next",
        default=False,
        help="Manually specify the CSS selector to use when looking for a next-page link.",
    )
    parser.add_option(
        "-i",
        "--index",
        dest="index",
        default=False,
        action="store_true",
        help="If passed, prepends a numeric index to each scraped filename, padded to 5 zeros.",
    )
    parser.add_option(
        "--si",
        "--starting-index",
        dest="start_at",
        default=0,
        help="Set the starting index value",
    )
    parser.add_option(
        "-t",
        "--timestamp",
        dest="timestamp",
        default=False,
        action="store_true",
        help="If passed, prepends a timestamp to each scraped filename.",
    )
    parser.add_option(
        "-d",
        "--delay",
        dest="delay",
        default=False,
        help="Manually specify the number of seconds to pause before downloading each target.",
    )
    parser.add_option(
        "-v",
        "--verbose",
        dest="verbose",
        default=False,
        action="store_true",
        help="If passed, shows extended logging information while downloading.",
    )
    parser.add_option(
        "-q",
        "--quiet",
        dest="quiet",
        default=False,
        action="store_true",
        help="If passed, scrapes silently.",
    )
    parser.add_option(
        "-f",
        "--format",
        dest="format",
        default="[width<=2160]",
        help="""Specify the format of downloaded videos. Passed on to youtube-dl.""",
    )
    (options, args) = parser.parse_args()

    if options.index:
        index_fnames = True
    if options.start_at:
        index_offset = int(options.start_at)
    if options.timestamp:
        timestamp_fnames = True

    if options.verbose:
        log.setLevel(logging.DEBUG)
    elif options.quiet:
        log.setLevel(logging.CRITICAL)
    else:
        log.setLevel(25)

    for uri in args:
        if options.selector:
            stream = getStream(
                uri, options.selector, nextLink=options.next, delay=int(options.delay)
            )
        else:
            stream = main(uri)
        processStream(stream, dir=options.directory)
